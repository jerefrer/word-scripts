#!/usr/bin/env python3
"""Detect Tibetan paragraphs whose phonetics no longer match generated output."""

from __future__ import annotations

import argparse
import shutil
import subprocess
import sys
import tempfile
from pathlib import Path
from typing import Iterable, List, TextIO, Tuple

try:
    from docx import Document
    from docx.enum.style import WD_STYLE_TYPE
    from docx.oxml import OxmlElement
    from docx.text.paragraph import Paragraph
except ImportError as error:  # pragma: no cover - CLI ergonomics
    sys.stderr.write(
        "python-docx is required. Install it with `pip install python-docx` before running this script.\n"
    )
    raise

SCRIPT_DIR = Path(__file__).resolve().parent
LINGUA_DIR = SCRIPT_DIR / "Lingua-BO-Wylie"
PERL_LIB_PATH = LINGUA_DIR / "lib"
PERL_SCRIPT_PATH = LINGUA_DIR / "bin" / "pronounce.pl"
DEFAULT_TIBETAN_STYLE = "Verse Tibetan"
DEFAULT_PHONETICS_STYLE = "Verse Phonetics"
DEFAULT_MANTRA_STYLE = "Mantra  Tibetan"
DEFAULT_SKIP_CHAR_STYLE = "Yigchung Tibetan Characters"
DEFAULT_LOG_FILE = "find-broken-phonetics.log"
DEFAULT_MANTRA_CHAR_STYLE = "Sanskrit at the beginning"
GYUR_SUFFIX = " gyur"
GYUR_TRIGGER_PHRASES = {
    "མོས་ལ",
    "བལྟས་ལ",
    "བསམ",
    "བསམ་ལ",
    "བསམ་མོ",
    "བསམ་ཞིང",
    "བསམ་ཞིང་",
    "བསམ་ལ་བཟླ་ཞིང",
    "བསམ་ལ་བཟླ་ཞིང་",
    "གསལ་གདབ",
    "མོས་པའི་ང་རྒྱལ་བསྐྱེད་དེ",
}
MANTRA_PREFIXES = {
    "ཨོཾ་ཨཱཿཧཱུྃ",
    "ཧཱུྃ་ཧཱུྃ་ཧཱུྃ",
    "ཧཱུྃ",
    "ན་མོ",
    "ཨོཾ",
    "བྷྲཱུ",
    "རཾ་ཡཾ་ཁཾ",
    "ཛཿཧཱུྃ་བཾ་ཧོཿ",
}
MAX_MANTRA_SYLLABLES = 3
TIBETAN_ENDING_PUNCTUATION = "།༎༑༏༐༔"
TIBSKRIT_SCRIPT = SCRIPT_DIR / "tibskrit-transliterator-cli.js"
_TIBSKRIT_WARNING_EMITTED = False
W_NS = {"w": "http://schemas.openxmlformats.org/wordprocessingml/2006/main"}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Scan DOCX files for Tibetan paragraphs whose adjacent phonetics paragraphs do not "
            "match freshly generated phonetics."
        )
    )
    parser.add_argument("paths", nargs="+", help="Files or folders to scan for .docx documents")
    parser.add_argument("--tibetan-style", default=DEFAULT_TIBETAN_STYLE, dest="tibetan_style")
    parser.add_argument("--phonetics-style", default=DEFAULT_PHONETICS_STYLE, dest="phonetics_style")
    parser.add_argument("--mantra-style", default=DEFAULT_MANTRA_STYLE, dest="mantra_style")
    parser.add_argument(
        "--skip-char-style",
        default=DEFAULT_SKIP_CHAR_STYLE,
        dest="skip_char_style",
        help=(
            "Character style whose text should be excluded from phonetics generation. "
            "Pass an empty string to disable."
        ),
    )
    parser.add_argument(
        "--log-file",
        default=DEFAULT_LOG_FILE,
        dest="log_file",
        help="Path to write mismatch details (default: %(default)s)",
    )
    parser.add_argument(
        "--mantra-char-style",
        default=DEFAULT_MANTRA_CHAR_STYLE,
        dest="mantra_char_style",
        help=(
            "Character style applied to transliterated mantra prefixes (default: %(default)s). "
            "Pass an empty string to disable styling."
        ),
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help=(
            "Prompt before fixing each mismatch. When enabled, the script will not advance "
            "until you confirm whether to overwrite a phonetics paragraph."
        ),
    )
    parser.add_argument(
        "--no-mantra",
        action="store_true",
        help="Disable mantra style check (treat mantra paragraphs like any other).",
    )
    return parser.parse_args()


def collect_docx_files(paths: Iterable[str]) -> List[Path]:
    docx_files: List[Path] = []
    for raw_path in paths:
        path = Path(raw_path).expanduser().resolve()
        if path.is_file():
            if _is_valid_docx(path):
                docx_files.append(path)
            else:
                sys.stderr.write(f"Skipped {path}: not a valid .docx file.\n")
        elif path.is_dir():
            for file_path in sorted(path.rglob("*.docx")):
                if _is_valid_docx(file_path):
                    docx_files.append(file_path.resolve())
        else:
            sys.stderr.write(f"Skipped {path}: not a .docx file or directory.\n")
    return docx_files


def _is_valid_docx(path: Path) -> bool:
    return path.suffix.lower() == ".docx" and not path.name.startswith("~$")


def ensure_perl_prerequisites() -> None:
    if shutil.which("perl") is None:
        raise RuntimeError("Perl is not available on PATH. Install it (e.g. `brew install perl`).")
    if not PERL_SCRIPT_PATH.exists():
        raise FileNotFoundError(f"Perl script not found at {PERL_SCRIPT_PATH}")
    if not PERL_LIB_PATH.exists():
        raise FileNotFoundError(f"Perl lib directory not found at {PERL_LIB_PATH}")


def generate_phonetics_for_group(text: str) -> str:
    if not text.strip():
        return ""
    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_dir_path = Path(tmp_dir)
        input_path = tmp_dir_path / "phonetics_input.txt"
        output_path = tmp_dir_path / "phonetics_output.txt"
        input_path.write_text(text, encoding="utf-8")
        command = [
            "perl",
            f"-I{PERL_LIB_PATH}",
            str(PERL_SCRIPT_PATH),
            "-sty",
            "padmakara-pt",
            str(input_path),
            str(output_path),
        ]
        subprocess.run(command, check=True, capture_output=True)
        if not output_path.exists():
            raise RuntimeError("Phonetics output file was not generated.")
        result = output_path.read_text(encoding="utf-8")
    sanitized = _clean_phonetics_text(result)
    if not sanitized:
        return ""
    return sanitized[0].upper() + sanitized[1:]


def _clean_phonetics_text(text: str) -> str:
    stripped = text.replace("\r", "").replace("\n", "").strip()
    return "".join(ch for ch in stripped if (ord(ch) >= 0x20) or ch == "\t")


def generate_phonetics_line(text: str) -> str:
    groups = [group for group in text.split() if group.strip()]
    phonetics_segments: List[str] = []
    for group in groups:
        phonetics = generate_phonetics_for_group(group)
        if phonetics:
            phonetics_segments.append(phonetics)
    return "   ".join(phonetics_segments)


def analyze_docx(
    path: Path,
    tibetan_style: str,
    phonetics_style: str,
    mantra_style: str | None,
    skip_char_style: str | None,
    mantra_char_style: str | None,
    log_handle: TextIO,
    interactive: bool,
) -> Tuple[int, int, bool]:
    document = Document(path)
    _ensure_mantra_char_style(document, mantra_char_style)
    paragraphs = document.paragraphs
    mismatches = 0
    fixed = 0
    skipped = False
    current_page = 1
    index = 0
    while index < len(paragraphs):
        paragraph = paragraphs[index]
        page_number = current_page
        current_page = _advance_page_counter(paragraph, current_page)
        paragraph_style = paragraph.style.name if paragraph.style is not None else ""
        if paragraph_style != tibetan_style:
            index += 1
            continue

        next_paragraph = paragraphs[index + 1] if index + 1 < len(paragraphs) else None
        next_style = next_paragraph.style.name if next_paragraph and next_paragraph.style else ""

        if mantra_style and next_style == mantra_style:
            index += 1
            continue

        segments = _build_phonetics_segments(
            paragraph,
            skip_char_style,
            mantra_char_style,
        )
        if not segments:
            index += 1
            continue
        generated = _segments_to_plain_text(segments)

        existing_paragraph = next_paragraph if next_style == phonetics_style else None
        existing_text = existing_paragraph.text.strip() if existing_paragraph else ""

        if existing_text.strip() != generated.strip():
            mismatches += 1
            _report_mismatch(
                log_handle,
                path,
                page_number,
                paragraph.text.strip(),
                existing_text.strip(),
                generated.strip(),
            )
            if interactive:
                fixed_current, skip_file = _maybe_fix_mismatch(
                    paragraph,
                    existing_paragraph,
                    phonetics_style,
                    segments,
                )
                if fixed_current:
                    fixed += 1
                if skip_file:
                    skipped = True
                    break

        index += 1
    if fixed:
        _safe_save_document(document, path)
    return mismatches, fixed, skipped


def _paragraph_text_for_phonetics(paragraph, skip_char_style: str | None) -> str:
    if not skip_char_style:
        return paragraph.text
    fragments: List[str] = []
    for run in paragraph.runs:
        text = run.text
        if not text:
            continue
        style_name = _style_name(getattr(run, "style", None))
        if style_name == skip_char_style:
            fragments.append(" ")
        else:
            fragments.append(text)
    return "".join(fragments)


def _style_name(style) -> str | None:
    if style is None:
        return None
    return getattr(style, "name", None) or str(style)


def _maybe_fix_mismatch(
    tibetan_paragraph,
    phonetics_paragraph,
    phonetics_style_name: str,
    segments: List[Tuple[str, str | None]],
) -> Tuple[bool, bool]:
    while True:
        prompt = "Fix this mismatch? [Y/n/s to skip file] "
        choice = input(prompt).strip().lower()
        if choice in {"", "y", "yes"}:
            if phonetics_paragraph is None:
                phonetics_paragraph = _insert_paragraph_after(
                    tibetan_paragraph,
                    "",
                    phonetics_style_name,
                )
            _apply_segments_to_paragraph(
                phonetics_paragraph,
                phonetics_style_name,
                segments,
            )
            return True, False
        if choice in {"n", "no"}:
            return False, False
        if choice in {"s", "skip"}:
            print("Skipping the remainder of this file...")
            return False, True
        print("Please respond with 'Y', 'n', or 's'.")


def _insert_paragraph_after(paragraph: Paragraph, text: str, style_name: str | None) -> Paragraph:
    new_p = OxmlElement("w:p")
    paragraph._p.addnext(new_p)
    new_paragraph = Paragraph(new_p, paragraph._parent)
    if style_name:
        new_paragraph.style = style_name
    if text:
        new_paragraph.add_run(text)
    return new_paragraph


def _apply_segments_to_paragraph(
    paragraph: Paragraph,
    style_name: str,
    segments: List[Tuple[str, str | None]],
) -> None:
    if paragraph.style is None or paragraph.style.name != style_name:
        paragraph.style = style_name
    for run in list(paragraph.runs):
        paragraph._p.remove(run._r)
    for text, char_style in segments:
        if not text:
            continue
        run = paragraph.add_run(text)
        if char_style:
            run.style = char_style


def _needs_gyur_suffix(paragraph: Paragraph, skip_char_style: str | None) -> bool:
    if not skip_char_style:
        return False
    for run in reversed(paragraph.runs):
        text = run.text.strip()
        if not text:
            continue
        style_name = _style_name(getattr(run, "style", None))
        if style_name != skip_char_style:
            return False
        normalized = _strip_tibetan_trailing_punctuation(text)
        return normalized in GYUR_TRIGGER_PHRASES
    return False


def _strip_tibetan_trailing_punctuation(text: str) -> str:
    stripped = text.rstrip(TIBETAN_ENDING_PUNCTUATION)
    return stripped.strip()


def _strip_tibetan_punctuation(text: str) -> str:
    return text.strip(TIBETAN_ENDING_PUNCTUATION + " ")


def _ensure_mantra_char_style(document: Document, style_name: str | None) -> None:
    if not style_name:
        return
    try:
        document.styles[style_name]
        return
    except KeyError:
        pass
    style = document.styles.add_style(style_name, WD_STYLE_TYPE.CHARACTER)
    style.font.italic = True


def _build_phonetics_segments(
    paragraph: Paragraph,
    skip_char_style: str | None,
    mantra_char_style: str | None,
) -> List[Tuple[str, str | None]]:
    source_text = _paragraph_text_for_phonetics(paragraph, skip_char_style)
    if not source_text.strip():
        return []
    mantra_prefix, remainder = _split_mantra_prefix(source_text)
    segments: List[Tuple[str, str | None]] = []
    if mantra_prefix:
        mantra_phonetics = _transliterate_mantra_with_tibskrit(mantra_prefix)
        if mantra_phonetics:
            char_style = mantra_char_style if mantra_char_style else None
            segments.append((mantra_phonetics.strip(), char_style))
    rest_source = remainder if mantra_prefix else source_text
    rest_phonetics = generate_phonetics_line(rest_source) if rest_source.strip() else ""
    if mantra_prefix and rest_phonetics.strip():
        segments.append(("   ", None))
    if rest_phonetics.strip():
        segments.append((rest_phonetics.strip(), None))
    if segments and _needs_gyur_suffix(paragraph, skip_char_style):
        _append_suffix_to_segments(segments, GYUR_SUFFIX)
    return segments


def _segments_to_plain_text(segments: List[Tuple[str, str | None]]) -> str:
    return "".join(text for text, _ in segments)


def _split_mantra_prefix(text: str) -> Tuple[str | None, str]:
    stripped = text.strip()
    if not stripped:
        return None, ""
    parts = stripped.split(None, 2)
    if len(parts) < 2:
        return None, stripped
    prefix_raw = parts[0]
    rest = stripped[len(prefix_raw):].lstrip()
    prefix = _strip_tibetan_punctuation(prefix_raw)
    if prefix and _is_short_mantra(prefix):
        return prefix, rest
    return None, stripped


def _transliterate_mantra_with_tibskrit(text: str) -> str:
    if not text.strip():
        return ""
    fallback = generate_phonetics_line(text).strip()
    with tempfile.NamedTemporaryFile("w", encoding="utf-8", delete=False) as tmp_file:
        tmp_file.write(text)
        tmp_path = Path(tmp_file.name)
    command = ["node", str(TIBSKRIT_SCRIPT), str(tmp_path), "false"]
    try:
        result = subprocess.run(
            command,
            check=True,
            capture_output=True,
            text=True,
        )
        return result.stdout.strip() or fallback
    except (subprocess.CalledProcessError, FileNotFoundError, PermissionError) as error:
        _emit_tibskrit_warning(error)
        return fallback
    finally:
        if tmp_path.exists():
            tmp_path.unlink()


def _emit_tibskrit_warning(error: Exception) -> None:
    global _TIBSKRIT_WARNING_EMITTED
    if _TIBSKRIT_WARNING_EMITTED:
        return
    _TIBSKRIT_WARNING_EMITTED = True
    sys.stderr.write(
        f"Warning: tibskrit transliteration failed ({error}). Falling back to default phonetics.\n"
    )


def _is_short_mantra(text: str) -> bool:
    if text in MANTRA_PREFIXES:
        return True
    syllables = [part for part in text.split("་") if part.strip(TIBETAN_ENDING_PUNCTUATION + " ")]
    return 0 < len(syllables) <= MAX_MANTRA_SYLLABLES


def _append_suffix_to_segments(segments: List[Tuple[str, str | None]], suffix: str) -> None:
    for index in range(len(segments) - 1, -1, -1):
        text, style = segments[index]
        if not text.strip():
            continue
        if style is None:
            segments[index] = (text.rstrip() + suffix, style)
            return
    segments.append((suffix, None))


def _report_mismatch(
    log_handle: TextIO,
    doc_path: Path,
    page_number: int,
    tibetan_text: str,
    existing_text: str,
    generated_text: str,
) -> None:
    tibetan = tibetan_text.replace("\n", " ")
    existing = existing_text.replace("\n", " ") or "<missing>"
    generated = generated_text.replace("\n", " ") or "<empty>"
    message = (
        f"{doc_path} (page {page_number}): phonetics mismatch\n"
        f"  Tibetan : {tibetan}\n"
        f"  Existing: {existing}\n"
        f"  Generated: {generated}\n"
    )
    print(message)
    log_handle.write(message + "\n")


def _advance_page_counter(paragraph, current_page: int) -> int:
    breaks = len(paragraph._p.xpath(".//w:lastRenderedPageBreak"))
    breaks += len(paragraph._p.xpath(".//w:br[@w:type='page']"))
    return current_page + breaks


def _safe_save_document(document: Document, destination: Path) -> None:
    destination = Path(destination)
    destination.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile(
        prefix=f"{destination.stem}_",
        suffix=destination.suffix,
        dir=str(destination.parent),
        delete=False,
    ) as tmp_file:
        temp_path = Path(tmp_file.name)
    try:
        document.save(temp_path)
        temp_path.replace(destination)
    finally:
        if temp_path.exists():
            temp_path.unlink()


def main() -> None:
    args = parse_args()
    mantra_style = None if args.no_mantra else args.mantra_style
    skip_char_style = args.skip_char_style.strip() or None
    mantra_char_style = args.mantra_char_style.strip() or None
    ensure_perl_prerequisites()
    docx_files = collect_docx_files(args.paths)
    if not docx_files:
        sys.stderr.write("No .docx files found.\n")
        return

    log_path = Path(args.log_file).expanduser()
    log_path.parent.mkdir(parents=True, exist_ok=True)
    total_mismatches = 0
    total_fixes = 0
    with log_path.open("w", encoding="utf-8") as log_handle:
        log_handle.write("find-broken-phonetics report\n\n")
        for docx_path in docx_files:
            try:
                mismatches, fixed, skipped_file = analyze_docx(
                    docx_path,
                    args.tibetan_style,
                    args.phonetics_style,
                    mantra_style,
                    skip_char_style,
                    mantra_char_style,
                    log_handle,
                    args.interactive,
                )
                total_mismatches += mismatches
                total_fixes += fixed
                status = f"{docx_path}: {mismatches} mismatch(es) found"
                if fixed:
                    status += f", {fixed} fixed"
                if skipped_file:
                    status += " (skipped rest of file)"
                print(status)
            except subprocess.CalledProcessError as error:
                sys.stderr.write(f"Failed to generate phonetics for {docx_path}: {error}\n")
            except Exception as error:  # pragma: no cover - defensive
                sys.stderr.write(f"Error processing {docx_path}: {error}\n")

    if total_mismatches:
        message = f"Done. Found {total_mismatches} mismatch(es). Details written to {log_path}."
        if total_fixes:
            message += f" {total_fixes} mismatch(es) fixed interactively."
        print(message)
    else:
        print("Done. No mismatches detected.")
        log_path.unlink(missing_ok=True)


if __name__ == "__main__":
    main()
